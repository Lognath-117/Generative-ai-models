{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "GAZuYbzXCNiO",
        "outputId": "23284bdb-93b7-4ed8-97d7-2a587fa47d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bacd1beffe23286343.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bacd1beffe23286343.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import kagglehub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "\n",
        "# Step 1: Download dataset using kagglehub\n",
        "dataset_path = kagglehub.dataset_download(\"mmoreaux/environmental-sound-classification-50\")\n",
        "\n",
        "# Step 2: Locate CSV and audio files\n",
        "csv_file = None\n",
        "audio_dir = None\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file == \"esc50.csv\":\n",
        "            csv_file = os.path.join(root, file)\n",
        "    if \"audio\" in dirs:\n",
        "        audio_dir = os.path.join(root, \"audio\")\n",
        "\n",
        "assert csv_file is not None, \"esc50.csv not found.\"\n",
        "assert audio_dir is not None, \"Audio directory not found.\"\n",
        "\n",
        "# Step 3: Dataset loader\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, csv_file, audio_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.labels = sorted(self.df[\"category\"].unique())\n",
        "        self.label2idx = {label: idx for idx, label in enumerate(self.labels)}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        file_path = os.path.join(self.audio_dir, row[\"filename\"])\n",
        "        y, sr = librosa.load(file_path, sr=16000)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "        mfcc = np.mean(mfcc.T, axis=0)\n",
        "        label = self.label2idx[row[\"category\"]]\n",
        "        return torch.tensor(mfcc, dtype=torch.float32), torch.tensor(label)\n",
        "\n",
        "# Step 4: Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=40, hidden_size=128, output_size=50):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # [batch, seq=1, feature]\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        out = self.fc(hn.squeeze(0))\n",
        "        return out\n",
        "\n",
        "# Step 5: Load model\n",
        "labels = sorted(pd.read_csv(csv_file)[\"category\"].unique())\n",
        "model = LSTMModel(output_size=len(labels))\n",
        "model_path = \"lstm_audio.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# Step 6: Predict function\n",
        "def classify_sound(file):\n",
        "    y, sr = librosa.load(file, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mfcc = np.mean(mfcc.T, axis=0)\n",
        "    input_tensor = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "    predicted = torch.argmax(output).item()\n",
        "    return f\"Predicted class: {labels[predicted]}\"\n",
        "\n",
        "# Step 7: Gradio UI\n",
        "gr.Interface(\n",
        "    fn=classify_sound,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Audio\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Environmental Sound Classification (ESC-50 + LSTM)\"\n",
        ").launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6My-DC6yIuw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Rc-j5sV7A_E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zy3o56tD8lmh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCneM8lY4BkT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axEj0qui2wGx"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}