{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFuKfHMzQ3qi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=['I love my sister ','This  text is for  fun','both are true or false but both are same']"
      ],
      "metadata": {
        "id": "uaIpVmQORCYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences=tokenizer.texts_to_sequences(text)\n",
        "pad_seq=pad_sequences(sequences,padding='post')"
      ],
      "metadata": {
        "id": "G8ZM9MyaRCbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjXaAYNmRCdZ",
        "outputId": "2985badc-64f5-4e37-bbea-dbbde1d60ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'both': 1, 'are': 2, 'i': 3, 'love': 4, 'my': 5, 'sister': 6, 'this': 7, 'text': 8, 'is': 9, 'for': 10, 'fun': 11, 'true': 12, 'or': 13, 'false': 14, 'but': 15, 'same': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXK4hOubRChG",
        "outputId": "5e35fbe4-720b-48ee-cb14-b4048067635e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 4, 5, 6], [7, 8, 9, 10, 11], [1, 2, 12, 13, 14, 15, 1, 2, 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pad_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0p6L9LzRCkn",
        "outputId": "f5d59eb1-5514-44ca-c31c-9957dd48bcce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3  4  5  6  0  0  0  0  0]\n",
            " [ 7  8  9 10 11  0  0  0  0]\n",
            " [ 1  2 12 13 14 15  1  2 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Legends call me leo dude\"\n",
        "word_tokens=text.split()\n",
        "print(\"word_tokens:\",word_tokens)\n",
        "char_tokens=list(text)\n",
        "print(\"char_tokens:\",char_tokens)"
      ],
      "metadata": {
        "id": "u3pEDwwpRCxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7adc7332-1b29-45d0-d3d2-e67de144d04f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokens: ['Legends', 'call', 'me', 'leo', 'dude']\n",
            "char_tokens: ['L', 'e', 'g', 'e', 'n', 'd', 's', ' ', 'c', 'a', 'l', 'l', ' ', 'm', 'e', ' ', 'l', 'e', 'o', ' ', 'd', 'u', 'd', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "assignig gramatical categories like nouns verbs etc to the text and provide crucial syntactic information for understanding the sentence structure challenges ambiguity unknown words language specific challenges\n",
        "\n"
      ],
      "metadata": {
        "id": "b8hE2TMHCXTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
        "\n",
        "# Print each token and its part of speech\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdRcBVlSCWdT",
        "outputId": "a9e2bde7-cd22-484d-99b0-38653c5cfde6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET\n",
            "quick ADJ\n",
            "brown ADJ\n",
            "fox NOUN\n",
            "jumps VERB\n",
            "over ADP\n",
            "the DET\n",
            "lazy ADJ\n",
            "dog NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ada1af32"
      },
      "source": [],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71790016",
        "outputId": "9a8d2075-9025-494d-a981-3be02f8a180d"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Define your sentence\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Apply POS tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print the result\n",
        "print(pos_tags)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "blg9Cr39Fx-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}